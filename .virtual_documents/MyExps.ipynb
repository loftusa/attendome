import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm 
import pickle

import torch


!which python


import sys
from pathlib import Path
sys.path.insert(0, str(Path.cwd() / "src"))





# generate data
seq_len = 50
num_of_samples = 2048 
vocab_size = tokenizer.vocab_size
random_sequence = torch.randint(1, vocab_size, (num_of_samples, seq_len))
random_repetitive_sequence = torch.cat([random_sequence, random_sequence], dim=1)


model.eval()
model.config._attn_implementation = "eager"
batch_size = 16
with torch.no_grad():
    for i in tqdm(range(0, num_of_samples, batch_size), desc="Computing induction scores"):
        begin_index = i
        end_index = min(i + batch_size, num_of_samples)
        batch = random_repetitive_sequence[begin_index:end_index, :]
        input_data = {"input_ids": batch.to("cuda")}
        print(input_data['input_ids'].shape)
        result = model(**input_data, output_attentions=True)
        print(result.attentions)
        for layer in range(model.config.num_hidden_layers):
            layer_values = result.attentions[layer]
            print(layer_values)
            break
        break


for layer in range(36):
    for head in range(32):
        if induction_dataset['model_results'][0]['induction_scores'][layer][head] > 0.7:
            print(f"({layer}, {head})")





from datasets import load_dataset
# dataset = load_dataset("allenai/olmo-mix-1124", streaming=True)
# subset = dataset["train"].take(100)
dataset = load_dataset(
    "common-pile/comma_v0.1_training_dataset", 
    streaming=True,
    # trust_remote_code=True
)


# Convert first N examples to a list
num_of_samples = 100
subset_data = []
for i, example in enumerate(dataset['train']):
    if i >= num_of_samples:  # Stop after 100 examples
        break
    subset_data.append(example['text'].lstrip())


# load model and tokenizer
from attendome.dataset import ModelLoader 
model_name = "Qwen/Qwen3-Embedding-8B"
loader = ModelLoader("cuda")
model, tokenizer = loader.load_model(model_name)


import torch.nn as nn
model = nn.DataParallel(model, device_ids=[0, 1])


import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel

def last_token_pool(last_hidden_states: Tensor,
                 attention_mask: Tensor) -> Tensor:
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]


# extract attn maps for induction heads 
model.eval()
# Set up tokenizer for left padding
tokenizer.pad_token = tokenizer.eos_token  # Set pad token for left padding
tokenizer.padding_side = "left"  # Set left padding

batch_size = 256
seq_len = 150

all_embeddings = []
with torch.no_grad():
    for i in tqdm(range(0, num_of_samples, batch_size), desc="Extracting sentence embeddings"):
        begin_index = i
        end_index = min(i + batch_size, num_of_samples)
        current_batch_size = end_index - begin_index
        
        # Get batch of strings
        batch_strings = subset_data[begin_index:end_index]
        
        # Tokenize witeh left padding and truncation to 150 tokens
        batch_tokens = tokenizer(
            batch_strings,
            padding=True,
            truncation=True,
            max_length=seq_len,
            return_tensors="pt"
        )
        
        input_data = {
            "input_ids": batch_tokens["input_ids"].to("cuda"),
            "attention_mask": batch_tokens["attention_mask"].to("cuda")
        }
        
        outputs = model(**input_data)
        embeddings = last_token_pool(outputs.last_hidden_state.cpu(), batch_tokens['attention_mask'])
        all_embeddings.append(embeddings.cpu().numpy())


all_embeddings = np.concatenate(all_embeddings, axis=0)


np.save('./results/embeddings/embeddings_Qwen3-Embedding-8B_pile100000-150.npy', all_embeddings)





from datasets import load_dataset
# dataset = load_dataset("allenai/olmo-mix-1124", streaming=True)
# subset = dataset["train"].take(100)
dataset = load_dataset(
    "common-pile/comma_v0.1_training_dataset", 
    streaming=True,
    # trust_remote_code=True
)


# Convert first N examples to a list
num_of_samples = 100
subset_data = []
for i, example in enumerate(dataset['train']):
    if i >= num_of_samples:  # Stop after 100 examples
        break
    subset_data.append(example['text'].lstrip())


# load model and tokenizer
from attendome.dataset import ModelLoader 
# model_name = "Qwen/Qwen3-4B"
# model_name = "Qwen/Qwen3-8B"
# model_name = "meta-llama/Llama-3.1-8B-Instruct"
# model_name = "meta-llama/Llama-3.2-3B-Instruct"
# model_name = "Qwen/Qwen3-Embedding-8B"
# model_name = 'allenai/OLMo-2-1124-7B'
# model_name = 'EleutherAI/pythia-6.9b'
# model_name = 'mistralai/Mistral-7B-Instruct-v0.1'
# model_name = 'mistralai/Mistral-Small-24B-Instruct-2501'
model_name = "EleutherAI/gpt-j-6b"
# model_name = "EleutherAI/gpt-neox-20b"
# model_name = "meta-llama/Llama-2-7b-hf"
# model_name = "meta-llama/Llama-2-13b-hf"

loader = ModelLoader("cuda")
model, tokenizer = loader.load_model(model_name)


model.eval()
model.config._attn_implementation = "eager"
tokenizer.pad_token = tokenizer.eos_token  # Set pad token for left padding


# Test with a simple sequence to see if the issue persists
test_text = "The quick brown fox"

test_tokens = tokenizer(test_text, return_tensors="pt")
with torch.no_grad():
    test_result = model(**test_tokens.to("cuda"), output_attentions=True)


# from nnsight import LanguageModel
# model = LanguageModel(model_name, device_map='cuda')

# with torch.no_grad():
#     for i in tqdm(range(0, 100, 4), desc="Extracting attention maps"):
#         begin_index = i
#         end_index = min(i + 4, 100)
#         current_batch_size = end_index - begin_index
        
#         # Get batch of strings
#         batch_strings = subset_data[begin_index:end_index]
        
#         # Tokenize with padding and truncation
#         batch_tokens = model.tokenizer(
#             batch_strings,
#             padding=True,
#             truncation=True,
#             max_length=150,
#             return_tensors="pt",
#             return_attention_mask=True
#         )
        
#         input_data = {
#             "input_ids": batch_tokens["input_ids"].to("cuda"),
#             "attention_mask": batch_tokens["attention_mask"].to("cuda")
#         }
        
#         result = model(**input_data, output_attentions=True)
#         break


# tmp = []
# for layer_index in range(36):
#     for head_index in range(32):
#         _score = induction_dataset['model_results'][0]['induction_scores'][layer_index][head_index]
#         if _score > 0.7:
#             tmp.append((layer_index, head_index, _score))


# extract attn maps for induction heads 
model.eval()
model.config._attn_implementation = "eager"

# Set up tokenizer for left padding
tokenizer.pad_token = tokenizer.eos_token  # Set pad token for left padding
# tokenizer.padding_side = "left"  # Set left padding

batch_size = 4
seq_len = 150
# Dictionary to store attention maps: {layer: {head: tensor}}
attention_maps = {}
store_all = True
# store_list = [(13, 13)] # best induction head -  qwen-4b
# store_list = [(14, 12)] # random head
# store_list = [(2, 31), (13, 13), (13, 15), (20, 21), (20, 26), 
#               (22, 22), (22, 23), (24, 22), (29, 9), (29, 11)] + \
#             [(7, 5), (1, 19), (17, 22), (15, 24), , (8, 30), (6, 18), (35, 6), (5, 19), (27, 8), (2, 1), (1, 29), (6, 0), (14, 2), (14, 31), (32, 20), (1, 22), (12, 24), (27, 3), (14, 6), (28, 31), (17, 28), (0, 13), (10, 7), (27, 9), (21, 29), (9, 31), (13, 27), (21, 22), (5, 30), (24, 17), (6, 7), (23, 6), (22, 5), (17, 0), (2, 24), (29, 22), (34, 20), (8, 0), (24, 14)]

with torch.no_grad():
    for i in tqdm(range(0, num_of_samples, batch_size), desc="Extracting attention maps"):
        begin_index = i
        end_index = min(i + batch_size, num_of_samples)
        current_batch_size = end_index - begin_index
        
        # Get batch of strings
        batch_strings = subset_data[begin_index:end_index]
        
        # Tokenize with left padding and truncation to 150 tokens
        batch_tokens = tokenizer(
            batch_strings,
            padding=True,
            truncation=True,
            max_length=seq_len,
            return_tensors="pt",
            return_attention_mask=True  # Ensure this is explicit
        )
        
        input_data = {
            "input_ids": batch_tokens["input_ids"].to("cuda"),
            "attention_mask": batch_tokens["attention_mask"].to("cuda")
        }
        
        result = model(**input_data, output_attentions=True)

        if store_all:
            # Store attention maps for each layer and head
            for layer in range(model.config.num_hidden_layers):
                layer_values = result.attentions[layer]  # Shape: (batch_size, num_heads, seq_len, seq_len)
                
                if layer not in attention_maps:
                    attention_maps[layer] = {}
                
                for head in range(layer_values.shape[1]):  # num_heads
                    if head not in attention_maps[layer]:
                        attention_maps[layer][head] = []
                    
                    # Store attention map for this head across all samples in batch
                    head_attention = layer_values[:, head, :, :].cpu()  # (batch_size, seq_len, seq_len)
                    # attention_maps[layer][head][begin_index:end_index] = head_attention.reshape((current_batch_size, -1))
                    
                    attention_maps[layer][head].append(head_attention)
        else: # store selected heads
            for (layer, head) in store_list:
                if layer not in attention_maps:
                    attention_maps[layer] = {}
                if head not in attention_maps[layer]:
                    attention_maps[layer][head] = []
                head_attention = result.attentions[layer][:, head, :, :].cpu()  # (batch_size, seq_len, seq_len)
                attention_maps[layer][head].append(head_attention)
                # attention_maps[layer][head][begin_index:end_index] = head_attention.reshape((current_batch_size, -1))

        break


torch.isnan(result.attentions[9][:,0].sum())


result.attentions[9][0,0]


batch_strings[0]


batch_tokens = tokenizer(batch_strings, padding=True, truncation=True, 
                        max_length=seq_len, return_tensors="pt")
print("First few token IDs:", batch_tokens["input_ids"][0][:20])
print("First few tokens:", tokenizer.convert_ids_to_tokens(batch_tokens["input_ids"][0][:20]))
print("Attention mask:", batch_tokens["attention_mask"][0][:20])


# # Add this after extracting attention maps to identify problematic samples
# for layer in attention_maps:
#     for head in attention_maps[layer]:
#         attention_tensor = torch.cat(attention_maps[layer][head], dim=0)
#         nan_mask = torch.isnan(attention_tensor)
#         if nan_mask.any():
#             print(f"Layer {layer}, Head {head}: {nan_mask.sum()} NaN values")
#             # Find which samples have NaNs
#             samples_with_nan = nan_mask.any(dim=-1).any(dim=-1).nonzero().squeeze()
#             print(f"Samples with NaN: {samples_with_nan}")


# Concatenate all batches for each head
print("Concatenating attention maps...")
for layer in attention_maps:
    for head in attention_maps[layer]:
        attention_maps[layer][head] = torch.cat(attention_maps[layer][head], dim=0)

        n_sample = attention_maps[layer][head].shape[0]
        attention_maps[layer][head] = attention_maps[layer][head].reshape((n_sample, -1)).numpy()


model_name


import pickle
file_path = "./results/attention_maps/attn_maps_mistral-7b_pile-100-150.pkl"
with open(file_path, 'wb') as f:
    pickle.dump(attention_maps, f)
print(f"Attention maps saved to {file_path}")


print()








# read file
import pickle
file_path = f"./results/attention_maps/attn_maps_{model_name}_pile-100-150.pkl"
with open(file_path, 'rb') as file:
    attention_maps = pickle.load(file)


attention_maps.keys()


# compute RDM
import rsatoolbox
from tqdm import tqdm
all_rdms = {}
for layer in tqdm(attention_maps):
    all_rdms[layer] = {}
    for head in attention_maps[layer]:
        data = rsatoolbox.data.Dataset(attention_maps[layer][head])
        rdms = rsatoolbox.rdm.calc_rdm(data)
        all_rdms[layer][head] = rdms.dissimilarities.reshape(-1)


import pickle
# name: rdms_{model_name}_{dataset_name}-{num_of_samples}_{max_token}.pkl
file_path = "./results/attention_maps/rdms_olmo2-7b_pile-100-150.pkl"
with open(file_path, 'wb') as f:
    pickle.dump(all_rdms, f)
print(f"RDMs saved to {file_path}")





# load the induction dataset, which contains the induction scores
import json
with open('./data/induction_heads/induction_dataset.json', 'r') as f:
    induction_dataset = json.load(f)


# model_name_list =  [
#     # "mistral-7b",
#     # "mistral-24b",
#     "gpt-j-6b",
#     "gpt-neox-20b",
#     "llama-2-7b",
#     "llama-2-13b"]
model_name_list = ["gpt-j-6b", "llama-2-7b"]
all_rdms = {}
for model_name in model_name_list:
    file_path = f"./results/attention_maps/rdms_{model_name}_pile-9900-150.pkl"
    with open(file_path, 'rb') as file:
        all_rdms[model_name] = pickle.load(file)


def get_top_heads(model_name):
    if 'gpt-j' in model_name:
        top_heads = [(15, 5, 0.0587), (9, 14, 0.0584), (12, 10, 0.0526), (8, 1, 0.0445), (11, 0, 0.0445), (13, 13, 0.019), (8, 0, 0.0184), (14, 9, 0.016), (9, 2, 0.0127), (24, 6, 0.0113), (15, 11, 0.0092),
                     (6, 6, 0.0069), (14, 0, 0.0068), (17, 8, 0.0068), (21, 2, 0.0067), (10, 11, 0.0066), (11, 2, 0.0057), (17, 0, 0.0054), (20, 11, 0.0051), (23, 0, 0.0047), (20, 0, 0.0046), (15, 7, 0.0045),
                     (27, 2, 0.0045), (21, 15, 0.0044), (11, 4, 0.0044), (18, 6, 0.0043), (9, 6, 0.0042), (4, 12, 0.004), (11, 15, 0.004), (20, 2, 0.0036), (10, 0, 0.0035), (16, 9, 0.0031), (11, 14, 0.0031),
                     (12, 4, 0.003), (9, 7, 0.003), (18, 3, 0.003), (19, 5, 0.003), (22, 5, 0.0027), (25, 3, 0.0026), (18, 9, 0.0025)]
    elif 'llama-2-7b' in model_name:
        top_heads = [(14, 1, 0.0391), (11, 2, 0.0225), (9, 25, 0.02), (12, 15, 0.0196), (12, 28, 0.0191), (13, 7, 0.0171), (11, 18, 0.0152), (12, 18, 0.0113), (16, 10, 0.007), (14, 16, 0.007),
                     (14, 14, 0.0048), (16, 1, 0.0042), (18, 1, 0.0042), (19, 16, 0.0041), (13, 30, 0.0034), (18, 26, 0.0032), (14, 7, 0.0032), (16, 0, 0.0031), (16, 29, 0.003), (29, 30, 0.003),
                     (16, 6, 0.0029), (15, 11, 0.0027), (12, 11, 0.0026), (11, 22, 0.0023), (16, 19, 0.0021), (15, 23, 0.002), (16, 20, 0.0019), (15, 9, 0.0019), (17, 28, 0.0019), (14, 18, 0.0018),
                     (8, 26, 0.0018), (29, 26, 0.0018), (15, 8, 0.0018), (13, 13, 0.0017), (30, 9, 0.0017), (13, 23, 0.0017), (13, 10, 0.0016), (11, 30, 0.0016), (12, 26, 0.0015), (19, 27, 0.0015),
                     (14, 9, 0.0014), (14, 10, 0.0013), (31, 17, 0.0013), (31, 4, 0.0013), (15, 17, 0.0013), (10, 5, 0.0012), (14, 11, 0.0012), (19, 12, 0.0012), (16, 7, 0.0012), (15, 24, 0.0011),
                     (26, 28, 0.0011), (11, 15, 0.0011), (15, 25, 0.0011), (17, 12, 0.0011), (13, 2, 0.0011), (14, 5, 0.0011), (14, 3, 0.001), (26, 30, 0.001), (27, 29, 0.001), (25, 12, 0.0009),
                     (15, 13, 0.0009), (10, 14, 0.0009), (28, 13, 0.0009), (17, 19, 0.0008), (19, 2, 0.0008), (12, 23, 0.0008), (15, 26, 0.0008), (28, 21, 0.0008), (15, 10, 0.0008), (12, 0, 0.0007),
                     (6, 16, 0.0007), (7, 28, 0.0007), (27, 7, 0.0007), (11, 28, 0.0007), (29, 15, 0.0006), (13, 8, 0.0006), (13, 17, 0.0006), (8, 0, 0.0006), (22, 17, 0.0006), (22, 20, 0.0006), 
                     (12, 2, 0.0006), (26, 9, 0.0006), (31, 26, 0.0006), (22, 27, 0.0005), (16, 26, 0.0005), (13, 1, 0.0005), (26, 2, 0.0005), (30, 10, 0.0005), (11, 25, 0.0005), (29, 20, 0.0005),
                     (19, 15, 0.0005), (12, 10, 0.0005), (12, 3, 0.0005), (30, 5, 0.0004), (6, 9, 0.0004), (15, 16, 0.0004), (23, 28, 0.0004), (22, 5, 0.0004), (31, 19, 0.0004), (26, 14, 0.0004)]
    elif 'llama-2-13b' in model_name:
        top_heads = [(13, 13, 0.0402), (12, 17, 0.0332), (15, 38, 0.0269), (14, 34, 0.0209), (19, 2, 0.0116), (19, 36, 0.0106), (13, 4, 0.0106), (18, 11, 0.01), (10, 15, 0.0087), (13, 23, 0.0077),
                     (14, 7, 0.0074), (15, 36, 0.0046), (12, 8, 0.0046), (17, 7, 0.0044), (38, 29, 0.0043), (15, 32, 0.0037), (17, 18, 0.0034), (16, 9, 0.0033), (14, 23, 0.0032), (39, 13, 0.0029),
                     (39, 14, 0.0027), (18, 22, 0.0026), (21, 32, 0.0026), (15, 18, 0.0026), (13, 14, 0.0026), (11, 31, 0.0025), (14, 39, 0.0024), (19, 14, 0.0023), (36, 23, 0.0021), (21, 7, 0.0021),
                     (8, 23, 0.002), (18, 18, 0.002), (17, 28, 0.002), (17, 9, 0.0019), (13, 27, 0.0017), (13, 34, 0.0017), (13, 12, 0.0016), (21, 2, 0.0016), (16, 16, 0.0015), (15, 31, 0.0015),
                     (26, 35, 0.0015), (10, 18, 0.0014), (11, 27, 0.0014), (13, 25, 0.0014), (15, 26, 0.0013), (5, 32, 0.0013), (20, 12, 0.0013), (18, 15, 0.0013), (16, 23, 0.0013), (25, 5, 0.0013),
                     (34, 6, 0.0012), (15, 2, 0.0012), (15, 27, 0.0012), (18, 20, 0.0012), (16, 19, 0.0011), (37, 4, 0.001), (19, 7, 0.001), (19, 3, 0.0009), (38, 14, 0.0009), (20, 21, 0.0009),
                     (21, 30, 0.0009), (16, 11, 0.0009), (13, 24, 0.0009), (9, 31, 0.0008), (14, 13, 0.0008), (16, 29, 0.0008), (15, 17, 0.0008), (19, 6, 0.0008), (23, 36, 0.0008), (18, 17, 0.0007),
                     (15, 34, 0.0007), (14, 29, 0.0007), (15, 7, 0.0007), (13, 17, 0.0007), (20, 11, 0.0007), (35, 16, 0.0007), (39, 27, 0.0007), (29, 27, 0.0006), (30, 24, 0.0006), (19, 37, 0.0006),
                     (39, 21, 0.0006), (13, 36, 0.0006), (37, 30, 0.0006), (16, 36, 0.0006), (15, 3, 0.0006), (19, 13, 0.0006), (13, 10, 0.0006), (14, 19, 0.0005), (36, 3, 0.0005), (15, 25, 0.0005),
                     (16, 0, 0.0005), (16, 10, 0.0005), (20, 29, 0.0005), (25, 13, 0.0005), (14, 36, 0.0005), (36, 7, 0.0005), (17, 0, 0.0005), (11, 37, 0.0005), (23, 18, 0.0005), (35, 10, 0.0005)]
    elif 'gpt-neox' in model_name:
        top_heads = [(9, 42, 0.0293), (12, 4, 0.0224), (9, 28, 0.019), (11, 57, 0.0079), (10, 43, 0.0073), (12, 14, 0.0069), (14, 31, 0.0065), (9, 23, 0.0057), (11, 21, 0.0054), (11, 4, 0.0052),
                     (9, 21, 0.0052), (18, 23, 0.005), (13, 9, 0.0048), (14, 49, 0.0048), (12, 20, 0.0047), (8, 30, 0.0045), (12, 59, 0.0043), (16, 42, 0.0039), (11, 34, 0.0038), (9, 33, 0.0038),
                     (9, 3, 0.0036), (11, 48, 0.0035), (14, 63, 0.0032), (18, 11, 0.0032), (13, 7, 0.003), (9, 27, 0.0029), (11, 23, 0.0029), (16, 30, 0.0027), (10, 17, 0.0026), (9, 55, 0.0024),
                     (11, 38, 0.0024), (11, 59, 0.0024), (20, 8, 0.0024), (15, 42, 0.0023), (11, 47, 0.0023), (9, 15, 0.0023), (8, 47, 0.0023), (10, 40, 0.0023), (18, 18, 0.0022), (9, 1, 0.0021),
                     (13, 12, 0.0021), (14, 5, 0.002), (16, 18, 0.0019), (13, 63, 0.0019), (9, 20, 0.0018), (26, 38, 0.0018), (21, 60, 0.0017), (17, 55, 0.0016), (17, 30, 0.0016), (10, 56, 0.0015),
                     (12, 3, 0.0015), (10, 16, 0.0014), (10, 0, 0.0013), (15, 62, 0.0013), (12, 15, 0.0013), (9, 34, 0.0013), (12, 18, 0.0013), (23, 46, 0.0012), (16, 53, 0.0012), (11, 1, 0.0011),
                     (9, 2, 0.0011), (10, 27, 0.0011), (23, 54, 0.0011), (16, 54, 0.0011), (12, 30, 0.0011), (11, 14, 0.0011), (16, 44, 0.001), (14, 27, 0.001), (26, 31, 0.001), (15, 0, 0.001),
                     (13, 46, 0.001), (15, 57, 0.001), (15, 17, 0.001), (19, 12, 0.0009), (9, 49, 0.0009), (10, 7, 0.0009), (19, 46, 0.0009), (8, 21, 0.0009), (25, 24, 0.0008), (19, 29, 0.0008),
                     (12, 21, 0.0008), (8, 18, 0.0008), (12, 35, 0.0008), (9, 10, 0.0008), (19, 40, 0.0008), (38, 5, 0.0008), (13, 31, 0.0007), (10, 38, 0.0007), (10, 12, 0.0007), (11, 31, 0.0007),
                     (10, 1, 0.0007), (23, 15, 0.0007), (13, 40, 0.0007), (9, 5, 0.0007), (22, 33, 0.0007), (13, 36, 0.0006), (8, 32, 0.0006), (16, 21, 0.0006), (14, 11, 0.0006), (13, 61, 0.0006)]
    else:
        raise ValueError(f"unsupported model: {model_name}")

    top_heads_dict = {}
    for head in top_heads:
        top_heads_dict[(head[0], head[1])] = head[2]
        
    return top_heads_dict


def prepare_data_for_plotting_fv(all_rdms, model_name):
    """
    Extract RDM vectors and corresponding induction scores for all attention heads
    """
    rdms = all_rdms[model_name]
    top_heads_dict = get_top_heads(model_name)
    
    rdm_vectors = []
    scores = []
    head_labels = []
    
    for layer_idx, layer in enumerate(rdms.keys()):
        for head_idx, head in enumerate(rdms[layer].keys()):
            # Get RDM vector
            rdm_vector = rdms[layer][head]
            rdm_vectors.append(rdm_vector)
            
            # Get score
            if (layer_idx, head_idx) in top_heads_dict:
                score = top_heads_dict[(layer_idx, head_idx)]
            else:
                score = 0
            scores.append(score)
            
            # Create label for this head
            head_labels.append(f"L{layer_idx}-H{head_idx}")
            
    
    return np.array(rdm_vectors), np.array(scores), head_labels


def prepare_data_for_plotting_induction(all_rdms, model_name,
                                        model_index, induction_dataset):
    """
    Extract RDM vectors and corresponding induction scores for all attention heads
    """
    rdms = all_rdms[model_name]
    rdm_vectors = []
    induction_scores = []
    head_labels = []
    
    for layer_idx, layer in enumerate(rdms.keys()):
        for head_idx, head in enumerate(rdms[layer].keys()):
            # Get RDM vector
            rdm_vector = rdms[layer][head]
            rdm_vectors.append(rdm_vector)
            
            # Get induction score
            induction_score = induction_dataset['model_results'][model_index]['induction_scores'][layer_idx][head_idx]
            induction_scores.append(induction_score)
            
            # Create label for this head
            head_labels.append(f"L{layer_idx}_H{head_idx}")
    
    return np.array(rdm_vectors), np.array(induction_scores), head_labels


X.shape


all_X = []
all_y = []
all_head_labels = []
all_model_id_labels = []
all_data = {}
for model_id, model_name in tqdm(enumerate(model_name_list)):
    X, y, head_labels = prepare_data_for_plotting_fv(all_rdms, model_name)
    # X, y, head_labels = prepare_data_for_plotting_induction(all_rdms, model_name,
                                                           # model_id, induction_dataset)
    X = X[:, :1000]
    all_X.append(X)
    all_y.append(y)
    new_head_labels = [model_name+ "_" + str(item) for item in head_labels]
    all_model_id_labels += len(y) * [model_id]
    all_head_labels.append(new_head_labels)

    all_data[model_name] = (X, y)

all_model_id_labels = np.array(all_model_id_labels)

X_concat = np.concatenate(all_X, axis=0)
y_concat = np.concatenate(all_y)
head_name_concat = []
for head_names in all_head_labels:
    head_name_concat += head_names


# Perform dimensionality reduction
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import seaborn as sns

reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_concat)-1))
coords_2d = reducer.fit_transform(X_concat)
title_suffix = "t-SNE"

# Create the plot
plt.figure(figsize=(12, 8))

# Create scatter plot
for i in range(2):
    scatter = plt.scatter(coords_2d[all_model_id_labels==i, 0], coords_2d[all_model_id_labels==i, 1], 
                         s=30, alpha=0.1, edgecolors='black', label=model_name_list[i])
    
plt.title("t-SNE of RDMs of Attn Heads")
plt.legend()
plt.show()


# X_qwen_4b, y_qwen_4b, head_name_qwen_4b = prepare_data_for_plotting(rdms_qwen_4b, 0, induction_dataset)
# X_qwen_8b, y_qwen_8b, head_name_qwen_8b = prepare_data_for_plotting(rdms_qwen_8b, 1, induction_dataset)
# X_llama_8b, y_llama_8b, head_name_llama_8b = prepare_data_for_plotting(rdms_llama_8b, 2, induction_dataset)


# head_name_qwen_4b = ["qwen3-4b_" + str(item) for item in head_name_qwen_4b]
# head_name_qwen_8b = ["qwen3-8b_" + str(item) for item in head_name_qwen_8b]
# head_name_llama_8b = ["llama3.1-8b_" + str(item) for item in head_name_llama_8b]


# X_concat = np.concatenate([X_qwen_4b,X_qwen_8b,X_llama_8b], axis=0)
# y_concat = np.concatenate([y_qwen_4b,y_qwen_8b,y_llama_8b])
# head_name_concat = head_name_qwen_4b + head_name_qwen_8b + head_name_llama_8b


# 3) Dimensionality reduction and plotting
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import seaborn as sns
def plot_attention_heads_2d(rdm_vectors, induction_scores, head_labels, method='tsne'):
    """
    Plot attention heads in 2D based on their RDMs, colored by induction scores
    """
    # Perform dimensionality reduction
    if method == 'tsne':
        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(rdm_vectors)-1))
        coords_2d = reducer.fit_transform(rdm_vectors)
        title_suffix = "t-SNE"
    elif method == 'pca':
        reducer = PCA(n_components=2, random_state=42)
        coords_2d = reducer.fit_transform(rdm_vectors)
        title_suffix = f"PCA (explained variance: {reducer.explained_variance_ratio_.sum():.2%})"
    else:
        raise ValueError("Method must be 'tsne' or 'pca'")
    
    # Create the plot
    plt.figure(figsize=(12, 8))
    
    # Create scatter plot
    scatter = plt.scatter(coords_2d[:, 0], coords_2d[:, 1], 
                         c=induction_scores, cmap='viridis', 
                         s=60, alpha=0.2, edgecolors='black', linewidth=0.5)
    
    # Add colorbar
    cbar = plt.colorbar(scatter)
    cbar.set_label('Induction Score', fontsize=12)
    
    # Customize plot
    plt.xlabel(f'{method.upper()} 1', fontsize=12)
    plt.ylabel(f'{method.upper()} 2', fontsize=12)
    plt.title(f'Attention Heads in RDM Space ({title_suffix})\nColored by Induction Score', fontsize=14)
    plt.grid(True, alpha=0.3)
    
    # Optionally add text labels for heads with highest/lowest induction scores
    # Get indices of top and bottom heads
    # top_indices = np.argsort(induction_scores)[-15:]  # Top 5
    top_indices = np.where(induction_scores>0.01)[0]

    for idx in top_indices:
        idx = int(idx)
        if head_labels[idx].startswith("gpt-j"):
            color = 'red'
        elif head_labels[idx].startswith("gpt-neox"):
            color = 'blue'
        elif head_labels[idx].startswith("llama-2-7b"):
            color = 'green'
        else:
            color = 'yellow'
        plt.annotate(head_labels[idx], (coords_2d[idx, 0], coords_2d[idx, 1]), 
                    xytext=(5, 5), textcoords='offset points', 
                    fontsize=12, alpha=0.8, color=color)
    
    # for idx in bottom_indices:
    #     plt.annotate(head_labels[idx], (coords_2d[idx, 0], coords_2d[idx, 1]), 
    #                 xytext=(5, 5), textcoords='offset points', 
    #                 fontsize=8, alpha=0.8, color='blue')
    
    plt.tight_layout()
    plt.show()
    
    return coords_2d


# Plot using t-SNE
print("Creating t-SNE visualization...")
tsne_coords = plot_attention_heads_2d(X_concat, y_concat, head_name_concat, method='tsne')


# Plot using PCA
print("Creating PCA visualization...")
pca_coords = plot_attention_heads_2d(X_concat, y_concat, head_name_concat, method='pca')


# # 4) Additional analysis: correlation between RDM similarity and induction scores
# def analyze_rdm_induction_correlation(rdm_vectors, induction_scores, head_labels):
#     """
#     Analyze the relationship between RDM patterns and induction scores
#     """
#     from scipy.stats import pearsonr
#     from scipy.spatial.distance import pdist, squareform
    
#     # Compute pairwise correlations between RDM vectors
#     rdm_similarity_matrix = np.corrcoef(rdm_vectors)
    
#     # Compute pairwise differences in induction scores
#     induction_diff_matrix = np.abs(np.subtract.outer(induction_scores, induction_scores))
    
#     # Extract upper triangle (to avoid duplicates and diagonal)
#     triu_indices = np.triu_indices_from(rdm_similarity_matrix, k=1)
#     rdm_similarities = rdm_similarity_matrix[triu_indices]
#     induction_diffs = induction_diff_matrix[triu_indices]
    
#     # Compute correlation
#     correlation, p_value = pearsonr(rdm_similarities, 1 - induction_diffs)  # 1 - diff to make it similarity
    
#     print(f"\nCorrelation Analysis:")
#     print(f"Correlation between RDM similarity and induction score similarity: {correlation:.3f}")
#     print(f"P-value: {p_value:.3e}")
    
#     # Plot the relationship
#     plt.figure(figsize=(10, 6))
#     plt.subplot(1, 2, 1)
#     plt.scatter(rdm_similarities, 1 - induction_diffs, alpha=0.5)
#     plt.xlabel('RDM Similarity (correlation)')
#     plt.ylabel('Induction Score Similarity (1 - |diff|)')
#     plt.title(f'RDM vs Induction Similarity\nr = {correlation:.3f}')
#     plt.grid(True, alpha=0.3)
    
#     # Plot induction score distribution
#     plt.subplot(1, 2, 2)
#     plt.hist(induction_scores, bins=20, alpha=0.7, edgecolor='black')
#     plt.xlabel('Induction Score')
#     plt.ylabel('Number of Heads')
#     plt.title('Distribution of Induction Scores')
#     plt.grid(True, alpha=0.3)
    
#     plt.tight_layout()
#     plt.show()

# # Run correlation analysis
# analyze_rdm_induction_correlation(rdm_vectors, induction_scores, head_labels)





# file_path = "./results/attention_maps/rdms_qwen3-8b_pile-100-150.pkl"
# with open(file_path, 'rb') as file:
#     rdms_qwen_8b = pickle.load(file)

# file_path = "./results/attention_maps/rdms_qwen3-4b_pile-100-150.pkl"
# with open(file_path, 'rb') as file:
#     rdms_qwen_4b = pickle.load(file)

# file_path = "./results/attention_maps/rdms_llama-3.2-3b_pile-100-150.pkl"
# with open(file_path, 'rb') as file:
#     rdms_llama_3b = pickle.load(file)

# file_path = "./results/attention_maps/rdms_llama-3.1-8b_pile-100-150.pkl"
# with open(file_path, 'rb') as file:
#     rdms_llama_8b = pickle.load(file)

# file_path = "./results/attention_maps/rdms_olmo2-7b_pile-100-150.pkl"
# with open(file_path, 'rb') as file:
#     rdms_olmo2_7b = pickle.load(file)

# file_path = "./results/attention_maps/rdms_pythia-6.9b_pile-100-150.pkl"
# with open(file_path, 'rb') as file:
#     rdms_pythia_69b = pickle.load(file)


# # for concept induction heads
# import json
# with open('./results/attention_scores/Llama-3.2-3B-Instruct/n2048_seqlen30.json', 'r') as file:
#     attention_score_llama_3b = json.load(file)
# with open('./results/attention_scores/Llama-3.1-8B-Instruct/n2048_seqlen30.json', 'r') as file:
#     attention_score_llama_8b = json.load(file)
# with open('./results/attention_scores/OLMo-2-1124-7B/n2048_seqlen30.json', 'r') as file:
#     attention_score_olmo2_7b = json.load(file)
# with open('./results/attention_scores/pythia-6.9b/n2048_seqlen30.json', 'r') as file:
#     attention_score_pythia_69b = json.load(file)


# def prepare_data_for_plotting(all_rdms, attention_score):
#     """
#     Extract RDM vectors and corresponding induction scores for all attention heads
#     """
#     rdm_vectors = []
#     induction_scores = []
#     head_labels = []
    
#     for layer_idx, layer in enumerate(all_rdms.keys()):
#         n_heads = len(all_rdms[layer])
#         for head_idx, head in enumerate(all_rdms[layer].keys()):
#             # Get RDM vector
#             rdm_vector = all_rdms[layer][head]
#             if np.isnan(rdm_vector.sum()):
#                 continue
#             rdm_vectors.append(rdm_vector)
            
#             # Get induction score next_tok_attn, end_tok_attn
#             induction_score = attention_score['end_tok_attn'][layer_idx * n_heads + head_idx]['score']
#             induction_scores.append(induction_score)
            
#             # Create label for this head
#             head_labels.append(f"L{layer_idx}_H{head_idx}")
    
#     return np.array(rdm_vectors), np.array(induction_scores), head_labels

# X_llama_3b, y_llama_3b, _ = prepare_data_for_plotting(rdms_llama_3b, attention_score_llama_3b)
# X_llama_8b, y_llama_8b, _ = prepare_data_for_plotting(rdms_llama_8b, attention_score_llama_8b)
# X_olmo2_7b, y_olmo2_7b, _ = prepare_data_for_plotting(rdms_olmo2_7b, attention_score_olmo2_7b)
# X_pythia_69b, y_pythia_69b, _ = prepare_data_for_plotting(rdms_pythia_69b, attention_score_pythia_69b)


# for induction heads
with open('./results/induction_heads/induction_dataset.json', 'r') as file:
    induction_dataset = json.load(file)
    
def prepare_data_for_plotting(all_rdms, model_index, induction_dataset):
    """
    Extract RDM vectors and corresponding induction scores for all attention heads
    """
    rdm_vectors = []
    induction_scores = []
    head_labels = []
    
    for layer_idx, layer in enumerate(all_rdms.keys()):
        for head_idx, head in enumerate(all_rdms[layer].keys()):
            # Get RDM vector
            rdm_vector = all_rdms[layer][head]
            
            # Get induction score
            induction_score = induction_dataset['model_results'][model_index]['induction_scores'][layer_idx][head_idx]
            
            if np.isnan(rdm_vector.sum()):
                print(f"L{layer_idx}_H{head_idx}, {induction_score}")
                continue
            rdm_vectors.append(rdm_vector)
            induction_scores.append(induction_score)
            head_labels.append(f"L{layer_idx}_H{head_idx}")
    
    return np.array(rdm_vectors), np.array(induction_scores), head_labels

X_olmo2_7b, y_olmo2_7b, _ = prepare_data_for_plotting(rdms_olmo2_7b, 0, induction_dataset)
X_pythia_69b, y_pythia_69b, _ = prepare_data_for_plotting(rdms_pythia_69b, 1, induction_dataset)
X_qwen_4b, y_qwen_4b, _ = prepare_data_for_plotting(rdms_qwen_4b, 2, induction_dataset)
X_qwen_8b, y_qwen_8b, _ = prepare_data_for_plotting(rdms_qwen_8b, 3, induction_dataset)
X_llama_3b, y_llama_3b, _ = prepare_data_for_plotting(rdms_llama_3b, 4, induction_dataset)
X_llama_8b, y_llama_8b, _ = prepare_data_for_plotting(rdms_llama_8b, 5, induction_dataset)


X_pythia_69b.shape, y_pythia_69b.shape


all_data = {"llama-3.2-3b": (X_llama_3b, y_llama_3b),
            "llama-3.1-8b": (X_llama_8b, y_llama_8b),
            "olmo2-7b": (X_olmo2_7b, y_olmo2_7b),
            "pythia-6.9b": (X_pythia_69b, y_pythia_69b)}
# "qwen3-4b": (X_qwen_4b, y_qwen_4b),
#             "qwen3-8b": (X_qwen_8b, y_qwen_8b),


all_data.keys()


all_data['llama-2-7b'][0].shape


import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt

X_train = all_data['llama-2-7b'][0]
y_train = all_data['llama-2-7b'][1]

# Train linear regression on 4b data
print("Training linear regression on train data...")

model = LinearRegression()
# model = Ridge(alpha=1)
model.fit(X_train, y_train)

# model = LogisticRegression(class_weight='balanced',max_iter=1000)
# y_train_thresholded = y_train>0.005
# model.fit(X_train, y_train_thresholded)

# bins = np.linspace(y_train.min(), y_train.max(), 10)
# y_binned = np.digitize(y_train, bins)
# from sklearn.utils.class_weight import compute_sample_weight
# sample_weights = compute_sample_weight('balanced', y_binned)
# model = LinearRegression()
# model.fit(X_train, y_train, sample_weight=sample_weights)


plt.scatter(model.predict(X_train), y_train)


threshold = 0.005


# Make predictions on test data
print("Making predictions on test data...")
X_test = all_data['gpt-j-6b'][0]
y_test = all_data['gpt-j-6b'][1]
# X_test = X_4b
# y_test = y_4b
y_pred = model.predict(X_test)
# y_pred = model.decision_function(X_test)

# Calculate performance metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print results
print(f"\nModel Performance:")
print(f"R² Score: {r2:.4f}")
print(f"Mean Squared Error: {mse:.4f}")
print(f"Root Mean Squared Error: {rmse:.4f}")
print(f"Mean Absolute Error: {mae:.4f}")

# Print model coefficients
print(f"\nModel Details:")
print(f"Number of features: {X_test.shape[1]}")
print(f"Training samples: {X_test.shape[0]}")
print(f"Test samples: {X_test.shape[0]}")

# Optional: Plot actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title(f'Actual vs Predicted Values\nR² = {r2:.4f}')
plt.grid(True, alpha=0.3)
plt.show()


# Binary classification evaluation (y > 0.5 = True)
print(f"\n" + "="*50)
print("BINARY CLASSIFICATION EVALUATION")
print("="*50)

# Convert to binary labels
y_binary_true = (y_test > threshold).astype(int)

# Use regression predictions as scores for AUROC
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
from sklearn.metrics import precision_recall_curve, roc_curve

# Calculate AUROC
auroc = roc_auc_score(y_binary_true, y_pred)
print(f"AUROC: {auroc:.4f}")

# Additional binary classification metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Class distribution
print(f"\nClass Distribution in Test Data:")
print(f"True (y > 0.35): {np.sum(y_binary_true)} ({np.mean(y_binary_true)*100:.1f}%)")
print(f"False (y ≤ 0.35): {len(y_binary_true) - np.sum(y_binary_true)} ({(1-np.mean(y_binary_true))*100:.1f}%)")

# Plot ROC Curve
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
fpr, tpr, _ = roc_curve(y_binary_true, y_pred)
plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUROC = {auroc:.4f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot Precision-Recall Curve
plt.subplot(1, 2, 2)
precision_vals, recall_vals, _ = precision_recall_curve(y_binary_true, y_pred)
from sklearn.metrics import average_precision_score
ap_score = average_precision_score(y_binary_true, y_pred)
plt.plot(recall_vals, precision_vals, linewidth=2, label=f'PR Curve (AP = {ap_score:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()


# Find optimal threshold using precision-recall or F1
from sklearn.metrics import precision_recall_curve
precision_vals, recall_vals, thresholds = precision_recall_curve(y_binary_true, y_pred)

# Find threshold that maximizes F1
f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals)
optimal_idx = np.argmax(f1_scores[:-1])  # exclude last element
optimal_threshold = thresholds[optimal_idx]

print(f"Optimal threshold: {optimal_threshold:.3f}")
print(f"F1 at optimal threshold: {f1_scores[optimal_idx]:.3f}")


(all_data['llama-2-7b'][1]>0.005).sum()


plt.plot(sorted(all_data['llama-2-7b'][1], reverse=True)[:100])


import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import roc_auc_score
import pandas as pd

threshold = 0.01

# Get dataset names
dataset_names = list(all_data.keys())
n_datasets = len(dataset_names)

# Initialize results matrix
auroc_matrix = np.zeros((n_datasets, n_datasets))
r2_matrix = np.zeros((n_datasets, n_datasets))

# Perform cross-dataset evaluation
for i, train_name in enumerate(dataset_names):
    print(f"training on {train_name}...")
    # Get training data
    X_train, y_train = all_data[train_name]
    # Train linear regression model
    # model = LinearRegression()
    model = Ridge(alpha=1)
    model.fit(X_train, y_train)
    
    for j, test_name in enumerate(dataset_names):
        if train_name == test_name:
            auroc_matrix[i, j] = np.nan
            r2_matrix[i, j] = np.nan
            continue
        print(f"testing on {test_name}...")
        # Get test data
        X_test, y_test = all_data[test_name]
        
        # Predict on test set
        y_pred = model.predict(X_test)
        
        # Convert to binary labels using threshold
        y_binary_true = (y_test > threshold).astype(int)
        
        # Calculate AUROC
        try:
            r2_matrix[i, j] = r2_score(y_test, y_pred)
            auroc = roc_auc_score(y_binary_true, y_pred)
            auroc_matrix[i, j] = auroc
        except ValueError as e:
            # Handle case where all labels are the same class
            print(f"Warning: AUROC calculation failed for {train_name}->{test_name}: {e}")
            auroc_matrix[i, j] = np.nan


# Create DataFrame for better visualization
auroc_df = pd.DataFrame(auroc_matrix, 
                       index=[f"Train: {name}" for name in dataset_names],
                       columns=[f"Test: {name}" for name in dataset_names])

# Create the heatmap
plt.figure(figsize=(8, 6))
mask = np.isnan(auroc_matrix)
sns.heatmap(auroc_df, 
            annot=True, 
            fmt='.3f', 
            cmap='RdYlBu_r',
            center=0.5,
            square=True,
            linewidths=0.5,
            cbar_kws={"shrink": .8},
            mask=mask)

plt.title(f'Function Vector Heads\nCross-Model Generalization AUROC\n(Linear Regression with threshold={threshold})', 
          fontsize=14, fontweight='bold')
plt.xlabel('Test Dataset', fontsize=12)
plt.ylabel('Training Dataset', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()


# Create DataFrame for better visualization
r2_df = pd.DataFrame(r2_matrix, 
                       index=[f"Train: {name}" for name in dataset_names],
                       columns=[f"Test: {name}" for name in dataset_names])

# Create the heatmap
plt.figure(figsize=(8, 6))
mask = np.isnan(auroc_matrix)
sns.heatmap(r2_df, 
            annot=True, 
            fmt='.3f', 
            cmap='RdYlBu_r',
            center=0.5,
            square=True,
            linewidths=0.5,
            cbar_kws={"shrink": .8},
            mask=mask)

plt.title('Cross-Dataset Generalization R2', 
          fontsize=14, fontweight='bold')
plt.xlabel('Test Dataset', fontsize=12)
plt.ylabel('Training Dataset', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()





from attendome.model import discriminative_models as dm


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


# load data
sent_emb = np.load("./results/embeddings/embeddings_Qwen3-Embedding-8B_pile100000-150.npy")[:10000]

file_path = "./results/attention_maps/attn_maps_qwen3-4b_50heads_pile-10000-150.pkl"
with open(file_path, 'rb') as f:
    attn_maps = pickle.load(f)


num_of_samples = sent_emb.shape[0]
num_of_samples


label = 0
labels = []
sentence_ids = []
layer_ids = []
head_ids = []
for layer_id in attn_maps.keys():
    for head_id in attn_maps[layer_id].keys():
        assert attn_maps[layer_id][head_id].shape[0] == num_of_samples, attn_maps[layer_id][head_id].shape
        for sent_id in range(num_of_samples):
            labels.append(label)
            sentence_ids.append(sent_id)
            layer_ids.append(layer_id)
            head_ids.append(head_id)
        label += 1

# Split data based on sent_id for generalization to new sentences
from sklearn.model_selection import train_test_split
# Get unique sentence IDs
unique_sent_ids = list(range(num_of_samples))
# Split sentence IDs into train and test sets
train_sent_ids, test_sent_ids = train_test_split(
    unique_sent_ids, test_size=0.2, random_state=42
)
# Split training sentence IDs into train and validation sets
train_sent_ids, val_sent_ids = train_test_split(
    train_sent_ids, test_size=0.2, random_state=42
)

# Create indices based on sentence ID splits
train_idx = [i for i, sent_id in enumerate(sentence_ids) if sent_id in train_sent_ids]
val_idx = [i for i, sent_id in enumerate(sentence_ids) if sent_id in val_sent_ids]
test_idx = [i for i, sent_id in enumerate(sentence_ids) if sent_id in test_sent_ids]

len(train_idx), len(val_idx), len(test_idx)


# Get dimensions
sent_emb_dim = len(sent_emb[0])
attn_map_dim = list(attn_maps[13][13])[0].flatten().shape[0]
num_classes = len(set(labels))

print(f"Dataset info:")
print(f"- Sentence embedding dim: {sent_emb_dim}")
print(f"- Attention map dim: {attn_map_dim}")
print(f"- Number of classes: {num_classes}")
print(f"- Train/Val/Test split: {len(train_idx)}/{len(val_idx)}/{len(test_idx)}")





# Create datasets
train_dataset = dm.AttentionHeadDataset(
    sent_emb, attn_maps,
    [sentence_ids[i] for i in train_idx],
    [layer_ids[i] for i in train_idx],
    [head_ids[i] for i in train_idx],
    [labels[i] for i in train_idx]
)

val_dataset = dm.AttentionHeadDataset(
    sent_emb, attn_maps,
    [sentence_ids[i] for i in val_idx],
    [layer_ids[i] for i in val_idx],
    [head_ids[i] for i in val_idx],
    [labels[i] for i in val_idx]
)

test_dataset = dm.AttentionHeadDataset(
    sent_emb, attn_maps,
    [sentence_ids[i] for i in test_idx],
    [layer_ids[i] for i in test_idx],
    [head_ids[i] for i in test_idx],
    [labels[i] for i in test_idx]
)

# Create data loaders
from torch.utils.data import Dataset, DataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


# Create supervised model
encoder = dm.AttentionHeadEncoder(sent_emb_dim, attn_map_dim, hidden_dim=512, output_dim=256)
supervised_model = dm.SupervisedClassifier(encoder, num_classes).to(device)

# Train supervised model
sup_train_losses, sup_val_losses, sup_val_accs = dm.train_supervised_model(
    supervised_model, train_loader, val_loader, device, epochs=50
)


plt.plot(sup_train_losses, label='train')
plt.plot(sup_val_losses, label='test')
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend()
plt.show()
plt.plot(sup_val_accs)
plt.ylabel("val acc")
plt.xlabel("epoch")
plt.show()


# Evaluate supervised model
sup_embeddings, sup_clusters, sup_metrics = dm.evaluate_embeddings(
    supervised_model, test_loader, device, [labels[i] for i in test_idx]
)

print(f"\nSupervised Learning Results:")
print(f"- Final validation accuracy: {sup_val_accs[-1]:.4f}")
print(f"- Silhouette score: {sup_metrics['silhouette_score']:.4f}")
print(f"- Adjusted Rand Index: {sup_metrics['adjusted_rand_index']:.4f}")


torch.save(supervised_model.state_dict(), './results/models/supervised_model.pth')





# Create contrastive datasets
contrastive_train_dataset = dm.ContrastiveDataset(
    sent_emb, attn_maps,
    [sentence_ids[i] for i in train_idx],
    [layer_ids[i] for i in train_idx],
    [head_ids[i] for i in train_idx],
    num_negatives=5
)

contrastive_val_dataset = dm.ContrastiveDataset(
    sent_emb, attn_maps,
    [sentence_ids[i] for i in val_idx],
    [layer_ids[i] for i in val_idx],
    [head_ids[i] for i in val_idx],
    num_negatives=5
)

# Create contrastive data loaders
contrastive_train_loader = DataLoader(contrastive_train_dataset, batch_size=16, shuffle=True)
contrastive_val_loader = DataLoader(contrastive_val_dataset, batch_size=16, shuffle=False)


# Create contrastive model
contrastive_model = dm.AttentionHeadEncoder(sent_emb_dim, attn_map_dim, hidden_dim=512, output_dim=256).to(device)

# Train contrastive model
cont_train_losses, cont_val_losses = dm.train_contrastive_model(
    contrastive_model, contrastive_train_loader, contrastive_val_loader, device, epochs=80
)


plt.plot(cont_train_losses, label='train')
plt.plot(cont_val_losses, label='val')
plt.legend()
plt.show()


# Evaluate contrastive model
cont_embeddings, cont_clusters, cont_metrics = dm.evaluate_embeddings(
    contrastive_model, test_loader, device, [labels[i] for i in test_idx]
)

print(f"\nContrastive Learning Results:")
print(f"- Silhouette score: {cont_metrics['silhouette_score']:.4f}")
print(f"- Adjusted Rand Index: {cont_metrics['adjusted_rand_index']:.4f}")


torch.save(contrastive_model.state_dict(), './results/models/contrastive_model.pth')


print(f"\nComparison of approaches:")
print(f"Supervised Learning:")
print(f"  - Silhouette Score: {sup_metrics['silhouette_score']:.4f}")
print(f"  - Adjusted Rand Index: {sup_metrics['adjusted_rand_index']:.4f}")
print(f"\nContrastive Learning:")
print(f"  - Silhouette Score: {cont_metrics['silhouette_score']:.4f}")
print(f"  - Adjusted Rand Index: {cont_metrics['adjusted_rand_index']:.4f}")

# Plot training curves
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(sup_train_losses, label='Train')
plt.plot(sup_val_losses, label='Validation')
plt.title('Supervised Learning Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(sup_val_accs)
plt.title('Supervised Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.subplot(1, 3, 3)
plt.plot(cont_train_losses, label='Train')
plt.plot(cont_val_losses, label='Validation')
plt.title('Contrastive Learning Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()





# load the induction dataset, which contains the induction scores
import json
with open('./results/induction_heads/induction_dataset.json', 'r') as f:
    induction_dataset = json.load(f)


# load sentence embedding
sent_emb = np.load("./results/embeddings/embeddings_Qwen3-Embedding-8B_pile100000-150.npy")[:10000]


# load attention map
file_path = "./results/attention_maps/attn_maps_qwen3-4b_pile-100-150.pkl"
with open(file_path, 'rb') as file:
    attn_maps_qwen4b = pickle.load(file)

file_path = "./results/attention_maps/attn_maps_qwen3-8b_pile-100-150.pkl"
with open(file_path, 'rb') as file:
    attn_maps_qwen8b = pickle.load(file)

file_path = "./results/attention_maps/attn_maps_llama-3.1-8b_pile-100-150.pkl"
with open(file_path, 'rb') as file:
    attn_maps_llama8b = pickle.load(file)


sent_emb = sent_emb[:100]
num_of_samples = sent_emb.shape[0]
sent_emb.shape


for layer_id in attn_maps_qwen4b.keys():
    for head_id in attn_maps_qwen4b[layer_id].keys():
        attn_maps_qwen4b[layer_id][head_id] = attn_maps_qwen4b[layer_id][head_id].numpy()


model_index = 0 # 0: qwen4b, 1: qwen8b, 2: llama8b
attn_maps = attn_maps_qwen4b

from collections import defaultdict

def group_and_average_embeddings(head_ids, embeddings):
    """
    Group embeddings by head IDs and compute average for each group.
    
    Args:
        head_ids: List of tuples representing attention head IDs
        embeddings: List of numpy arrays or lists representing embeddings
    
    Returns:
        Dictionary mapping head_id -> averaged embedding
    """
    # Group embeddings by head ID
    grouped_embeddings = defaultdict(list)
    
    for head_id, embedding in zip(head_ids, embeddings):
        grouped_embeddings[head_id].append(embedding)
    
    # Calculate average for each group
    averaged_embeddings = {}
    for head_id, embedding_list in grouped_embeddings.items():
        # Convert to numpy array and compute mean along axis 0
        embedding_array = np.array(embedding_list)
        averaged_embeddings[head_id] = np.mean(embedding_array, axis=0)
    
    return averaged_embeddings

def compute_average_emb(sent_emb, attn_maps, model_name, model):
    # get model index
    name2index = {"qwen3-4b": 0, 
                 "qwen3-8b": 1,
                 "llama3.2-8b": 2}
    model_index = name2index[model_name]
    
    labels = []
    sentence_ids = []
    layer_ids = []
    head_ids = []
    head_id_to_label = {}
    head_id_to_score = {}
    for layer_id in attn_maps.keys():
        for head_id in attn_maps[layer_id].keys():
            assert attn_maps[layer_id][head_id].shape[0] == num_of_samples, attn_maps[layer_id][head_id].shape
            # check if induction head
            induction_score = induction_dataset['model_results'][model_index]['induction_scores'][layer_id][head_id]
            if induction_score > 0.35:
                label = 1
            else:
                label = 0
            head_id_to_label[(layer_id, head_id)] = label
            head_id_to_score[(layer_id, head_id)] = induction_score
            
            for sent_id in range(num_of_samples):
                labels.append(label)
                sentence_ids.append(sent_id)
                layer_ids.append(layer_id)
                head_ids.append(head_id)

    # Create datasets
    cross_model_dataset = dm.AttentionHeadDataset(
        sent_emb, attn_maps,
        sentence_ids,
        layer_ids,
        head_ids,
        labels
    )
    
    # Create data loaders
    from torch.utils.data import Dataset, DataLoader
    cross_model_loader = DataLoader(cross_model_dataset, batch_size=32, shuffle=True)

    # extract embeddings    
    all_embeddings = []
    all_head_ids = []
    all_labels = []
    with torch.no_grad():
        for batch in tqdm(cross_model_loader):
            sent_emb_batch = batch['sent_embedding'].to(device)
            attn_map_batch = batch['attn_map'].to(device)
            
            if hasattr(model, 'encoder'):
                embeddings = model.encoder(sent_emb_batch, attn_map_batch)
            else:
                embeddings = model(sent_emb_batch, attn_map_batch)
            
            all_embeddings.append(embeddings.cpu().numpy())
            
            if 'label' in batch:
                all_labels.extend(batch['label'].cpu().numpy())
            
            # Store head IDs for analysis
            head_ids = [(l.item(), h.item()) for l, h in zip(batch['layer_id'], batch['head_id'])]
            all_head_ids.extend(head_ids)
    
    all_embeddings = np.vstack(all_embeddings)

    averaged_embeddings = group_and_average_embeddings(all_head_ids, all_embeddings)

    return averaged_embeddings, head_id_to_score, head_id_to_label


avg_embedding_qwen4b, head_id_to_score_qwen4b, head_id_to_label_qwen4b = compute_average_emb(sent_emb, attn_maps_qwen4b, 
                                                                                             "qwen3-4b", contrastive_model)
avg_embedding_qwen8b, head_id_to_score_qwen8b, head_id_to_label_qwen8b = compute_average_emb(sent_emb, attn_maps_qwen8b, 
                                                                                             "qwen3-8b", contrastive_model)
avg_embedding_llama8b, head_id_to_score_llama8b, head_id_to_label_llama8b = compute_average_emb(sent_emb, attn_maps_llama8b, 
                                                                                             "llama3.2-8b", contrastive_model)


def prepare_data(avg_emb, head_id_to_score):
    X = []
    y = []
    for head_id, embedding in avg_emb.items():
        X.append(embedding)
        y.append(head_id_to_score[head_id])
    X = np.array(X)
    y = np.array(y)
    return X, y
X_qwen4b, y_qwen4b = prepare_data(avg_embedding_qwen4b, head_id_to_score_qwen4b)
X_qwen8b, y_qwen8b = prepare_data(avg_embedding_qwen8b, head_id_to_score_qwen8b)
X_llama8b, y_llama8b = prepare_data(avg_embedding_llama8b, head_id_to_score_llama8b)
all_data = {"qwen3-4b": (X_qwen4b, y_qwen4b),
           "qwen3-8b": (X_qwen8b, y_qwen8b),
           "llama-3.2-8b": (X_llama8b, y_llama8b)}


import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import roc_auc_score
import pandas as pd
from sklearn.metrics import r2_score

# Get dataset names
dataset_names = list(all_data.keys())
n_datasets = len(dataset_names)

# Initialize results matrix
auroc_matrix = np.zeros((n_datasets, n_datasets))
r2_matrix = np.zeros((n_datasets, n_datasets))

# Perform cross-dataset evaluation
for i, train_name in enumerate(dataset_names):
    print(f"training on {train_name}...")
    # Get training data
    X_train, y_train = all_data[train_name]
    # Train linear regression model
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    for j, test_name in enumerate(dataset_names):
        if train_name == test_name:
            auroc_matrix[i, j] = np.nan
            r2_matrix[i, j] = np.nan
            continue
        print(f"testing on {test_name}...")
        # Get test data
        X_test, y_test = all_data[test_name]
        
        # Predict on test set
        y_pred = model.predict(X_test)
        
        # Convert to binary labels using threshold
        y_binary_true = (y_test > 0.35).astype(int)
        
        # Calculate AUROC
        try:
            r2_matrix[i, j] = r2_score(y_test, y_pred)
            auroc = roc_auc_score(y_binary_true, y_pred)
            auroc_matrix[i, j] = auroc
        except ValueError as e:
            # Handle case where all labels are the same class
            print(f"Warning: AUROC calculation failed for {train_name}->{test_name}: {e}")
            auroc_matrix[i, j] = np.nan


# Create DataFrame for better visualization
auroc_df = pd.DataFrame(auroc_matrix, 
                       index=[f"Train: {name}" for name in dataset_names],
                       columns=[f"Test: {name}" for name in dataset_names])

# Create the heatmap
plt.figure(figsize=(8, 6))
mask = np.isnan(auroc_matrix)
sns.heatmap(auroc_df, 
            annot=True, 
            fmt='.3f', 
            cmap='RdYlBu_r',
            center=0.5,
            square=True,
            linewidths=0.5,
            cbar_kws={"shrink": .8},
            mask=mask)

plt.title('Cross-Model Generalization AUROC\nContrastive Objective (InfoNCELoss) on 50 heads of Qwen3-4B\n(Linear Regression with threshold=0.35)', 
          fontsize=14, fontweight='bold')
plt.xlabel('Test Dataset', fontsize=12)
plt.ylabel('Training Dataset', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()





import numpy as np
import torch
import torch.nn as nn
import time

# Assuming you have your attention maps loaded as:
# attn_maps_a[layer_id][head_id] shape: [100, 22500]
# attn_maps_b[layer_id][head_id] shape: [100, 22500]

def prepare_data(name_1, name_2, train_size=80):
    """
    Prepare data for regression
    """
    file_path = f"./results/attention_maps/attn_maps_{name_1}_pile-100-150.pkl"
    with open(file_path, 'rb') as f:
        attn_maps_a = pickle.load(f)
    
    file_path = f"./results/attention_maps/attn_maps_{name_2}_pile-100-150.pkl"
    with open(file_path, 'rb') as f:
        attn_maps_b = pickle.load(f)
    
    num_layers_a = len(attn_maps_a)
    num_heads_a = len(attn_maps_a[0])
    num_layers_b = len(attn_maps_b)
    num_heads_b = len(attn_maps_b[0])
    num_samples = 100
    
    # Flatten all A heads into one array: [100, 1152, 22500]
    A_data = np.zeros((num_samples, num_layers_a * num_heads_a, 22500))
    B_data = np.zeros((num_samples, num_layers_b * num_heads_b, 22500))
    
    for layer in tqdm(range(num_layers_a)):
        for head in range(num_heads_a):
            head_idx = layer * num_heads_a + head
            A_data[:, head_idx, :] = attn_maps_a[layer][head]
            
    for layer in tqdm(range(num_layers_b)):
        for head in range(num_heads_b):
            head_idx = layer * num_heads_b + head
            B_data[:, head_idx, :] = attn_maps_b[layer][head]
    
    # Split into train/test
    train_indices = np.arange(train_size)
    test_indices = np.arange(train_size, num_samples)
    
    A_train = A_data[train_indices]  # [80, 1152, 22500]
    A_test = A_data[test_indices]    # [20, 1152, 22500]
    B_train = B_data[train_indices]  # [80, 1152, 22500]
    B_test = B_data[test_indices]    # [20, 1152, 22500]
    
    return A_train, A_test, B_train, B_test

def pytorch_approach(A_train, A_test, B_train, B_test, lr=0.001, epochs=1000):
    """
    Using PyTorch, training each B head separately to save GPU memory
    """
    print("Training with PyTorch approach (one head at a time)...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Convert A data to tensors (these stay the same for all heads)
    A_train_tensor = torch.FloatTensor(A_train).to(device)  # [80, 1152, 22500]
    A_test_tensor = torch.FloatTensor(A_test).to(device)    # [20, 1152, 22500]
    
    num_heads_a = A_train_tensor.shape[1]
    num_heads_b = B_train.shape[1]
    
    # Store results for all heads
    weights_all = np.zeros((num_heads_b, num_heads_a))
    bias_all = np.zeros(num_heads_b)
    train_errors = []
    test_errors = []
    
    # Model for predicting a single B head
    class SingleHeadRegression(nn.Module):
        def __init__(self, num_heads_a, num_positions=22500):
            super().__init__()
            self.weights = nn.Parameter(torch.randn(num_heads_a))
            self.bias = nn.Parameter(torch.zeros(22500))
            
        def forward(self, A):
            # A: [batch_size, num_heads_a, num_positions]
            # weights: [num_heads_a]
            # Output: [batch_size, num_positions]
            
            # Expand weights for broadcasting: [1, num_heads_a, 1]
            weights_expanded = self.weights.unsqueeze(0).unsqueeze(-1)
            
            # Weighted sum across heads: [batch_size, num_positions]
            output = (A * weights_expanded).sum(dim=1) + self.bias
            
            return output
    
    criterion = nn.MSELoss()
    
    # Train each B head separately
    for b_head in range(num_heads_b):
        print(f"Training head {b_head + 1}/{num_heads_b}")
        
        # Get target for this specific B head
        B_train_head = torch.FloatTensor(B_train[:, b_head, :]).to(device)  # [80, 22500]
        B_test_head = torch.FloatTensor(B_test[:, b_head, :]).to(device)    # [20, 22500]
        
        # Create new model for this head
        model = SingleHeadRegression(num_heads_a).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        
        # Training loop for this head
        for epoch in range(epochs):
            model.train()
            optimizer.zero_grad()
            
            # Forward pass
            predictions = model(A_train_tensor)
            loss = criterion(predictions, B_train_head)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            # Print progress every 200 epochs
            if (epoch + 1) % 50 == 0:
                model.eval()
                with torch.no_grad():
                    test_predictions = model(A_test_tensor)
                    test_loss = criterion(test_predictions, B_test_head)
                    print(f"  Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.6f}, Test Loss: {test_loss.item():.6f}")
        
        # Store final weights and bias for this head
        model.eval()
        with torch.no_grad():
            weights_all[b_head] = model.weights.cpu().numpy()
            bias_all[b_head] = model.bias.cpu().numpy()[0]
            
            # Calculate final errors for this head
            train_pred = model(A_train_tensor)
            test_pred = model(A_test_tensor)
            
            train_mse = criterion(train_pred, B_train_head).item()
            test_mse = criterion(test_pred, B_test_head).item()
            
            train_errors.append(train_mse)
            test_errors.append(test_mse)
            
            print(f"  Head {b_head + 1} final - Train MSE: {train_mse:.6f}, Test MSE: {test_mse:.6f}")
    
    return weights_all, bias_all, train_errors, test_errors


# Prepare data
print("Preparing data...")
# qwen3-4b, qwen3-8b, llama-3.1-8b
A_train, A_test, B_train, B_test = prepare_data("qwen3-4b", "llama-3.1-8b")

print(f"A_train shape: {A_train.shape}")
print(f"B_train shape: {B_train.shape}")


from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.metrics import mean_squared_error
import random
def sklearn_approach(A_train, A_test, B_train, B_test, k=10):
    """
    Using scikit-learn LinearRegression
    This reshapes the problem to treat each (sample, position) as a separate example
    """
    print("Training with scikit-learn approach...")
    
    num_train_samples, num_heads_a, num_positions = A_train.shape
    num_heads_b = B_train.shape[1]
    
    # Store results
    weights_all = np.zeros((num_heads_b, num_heads_a))
    intercepts_all = np.zeros(num_heads_b)
    train_errors = []
    test_errors = []
    
    # for b_head in range(num_heads_b):
    for b_head in random.sample(range(num_heads_b), k):
        print(f"Training head {b_head + 1}/{num_heads_b}")
        
        # Reshape for sklearn: treat each (sample, position) as separate training example
        X_train = A_train.transpose(0, 2, 1).reshape(-1, num_heads_a)  # [80*22500, 1152]
        y_train = B_train[:, b_head, :].flatten()  # [80*22500]
        
        X_test = A_test.transpose(0, 2, 1).reshape(-1, num_heads_a)  # [20*22500, 1152]
        y_test = B_test[:, b_head, :].flatten()  # [20*22500]
        
        # Fit linear regression
        # model = LinearRegression()
        model = SGDRegressor(max_iter=5000, tol=1e-6, penalty=None, verbose=1)
        model.fit(X_train, y_train)
        
        # Store weights and intercept
        weights_all[b_head] = model.coef_
        intercepts_all[b_head] = model.intercept_
        
        # Calculate errors
        train_pred = model.predict(X_train)
        test_pred = model.predict(X_test)
        
        train_mse = mean_squared_error(y_train, train_pred)
        test_mse = mean_squared_error(y_test, test_pred)
        
        train_errors.append(train_mse)
        test_errors.append(test_mse)
        print(f"Train mse: {train_mse:.6f}, test mse: {test_mse:.6f}")
    
    return weights_all, intercepts_all, train_errors, test_errors


weights, intercepts, train_errors, test_errors = sklearn_approach(
    A_train, A_test, B_train, B_test, k=100
)


# save npy
np.save("./results/naive-pred_weights_qwen4btollama8b.npy", weights)


# load npy
weights = np.load("./results/naive-pred_weights_qwen4btollama8b.npy")
intercepts = np.load("./results/naive-pred_intercepts_qwen4btollama8b.npy")


# compute test loss
selected_head_ids = np.where(weights.sum(axis=1) != 0)[0]

mse_loss_pred_list = []
mse_loss_same_head_list = []
for sent_id in tqdm(range(20)):
    for B_head_id in selected_head_ids:
        gt_map = B_test[sent_id][B_head_id].reshape((150, 150))
        same_head_map = A_test[sent_id][B_head_id].reshape((150, 150))
        predicted_map = (weights[B_head_id] @ A_test[sent_id]).reshape((150, 150)) + intercepts[B_head_id]
        
        mse_loss_pred = ((predicted_map - gt_map)**2).mean()
        mse_loss_same_head = ((same_head_map - gt_map)**2).mean()

        mse_loss_pred_list.append(mse_loss_pred)
        mse_loss_same_head_list.append(mse_loss_same_head)


mse_loss_pred_list_qwen4b2llama8b = mse_loss_pred_list
mse_loss_same_head_list_qwen4b2llama8b = mse_loss_same_head_list


np.mean(mse_loss_pred_list_qwen4b2llama8b), np.mean(mse_loss_same_head_list_qwen4b2llama8b)


np.mean(mse_loss_pred_list_qwen4b2qwen8b), np.mean(mse_loss_same_head_list_qwen4b2qwen8b)


# Organize data for plotting
categories = ['Qwen4B → Llama8B', 'Qwen4B → Qwen8B']
pred_values = [np.mean(mse_loss_pred_list_qwen4b2llama8b), np.mean(mse_loss_pred_list_qwen4b2qwen8b)]
same_head_values = [np.mean(mse_loss_same_head_list_qwen4b2llama8b), np.mean(mse_loss_same_head_list_qwen4b2qwen8b)]

# Set up the bar plot
x = np.arange(len(categories))
width = 0.35

fig, ax = plt.subplots(figsize=(10, 6))

# Create bars
bars1 = ax.bar(x - width/2, pred_values, width, label='Pred', alpha=0.8, color='skyblue')
bars2 = ax.bar(x + width/2, same_head_values, width, label='Same Head', alpha=0.8, color='lightcoral')

# Customize the plot
ax.set_xlabel('Model Comparison', fontsize=12)
ax.set_ylabel('MSE Loss', fontsize=12)
ax.set_title('Naive Pred vs Same Head (MSE)', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(categories)
ax.legend()

# Add value labels on bars
for bar in bars1:
    height = bar.get_height()
    ax.annotate(f'{height:.5f}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),  # 3 points vertical offset
                textcoords="offset points",
                ha='center', va='bottom')

for bar in bars2:
    height = bar.get_height()
    ax.annotate(f'{height:.5f}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),  # 3 points vertical offset
                textcoords="offset points",
                ha='center', va='bottom')

# Add grid for better readability
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()


input_model = 'qwen4b'
output_model = 'qwen8b'
sent_id = 2
B_head_id = 163
gt_map = B_test[sent_id][B_head_id].reshape((150, 150))
plt.imshow(gt_map)
plt.axis('off')
plt.title(f"Attn map for head {B_head_id}, test sent {sent_id}")
plt.show()


same_head_map = A_test[sent_id][B_head_id].reshape((150, 150))
plt.imshow(same_head_map)
plt.axis('off')
plt.title(f"Same position attn map for head {B_head_id}, test sent {sent_id}")
plt.show()


predicted_map = (weights[B_head_id] @ A_test[sent_id]).reshape((150, 150)) + intercepts[B_head_id]
plt.imshow(predicted_map)
plt.axis('off')
plt.title(f"Predicted map for head {B_head_id}, test sent {sent_id}")
plt.show()


((predicted_map - gt_map)**2).mean()


((same_head_map - gt_map)**2).mean()


np.argmax(weights[B_head_id]), np.max(weights[B_head_id])


# Train with PyTorch approach
start_time = time.time()
weights, bias, train_errors, test_errors = pytorch_approach(
    A_train, A_test, B_train, B_test, epochs=1500
)
end_time = time.time()

print(f"\nTraining completed in {end_time - start_time:.2f} seconds")
print(f"Average train MSE across all heads: {np.mean(train_errors):.6f}")
print(f"Average test MSE across all heads: {np.mean(test_errors):.6f}")
print(f"Best performing head (test MSE): {np.min(test_errors):.6f}")
print(f"Worst performing head (test MSE): {np.max(test_errors):.6f}")



